import streamlit as st
from dotenv import load_dotenv
import os
from langchain_core.prompts import ChatPromptTemplate
from langchain.prompts import MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma

# remembers last 6 chat elements
memory = 6

def init_chain():

    """Initialize the chat model and retrieval chain.

    This function sets up the environment variables, loads the OpenAI model,
    creates a ChromaDB retriever, and compiles the final chain for question answering.

    Returns:
        Runnable Sequence: The initialized retrieval chain.
    """

    # Load the environment variables and intialize the model
    load_dotenv(".env") 
    key = os.environ["OPENAI_KEY"]
    llm = ChatOpenAI(model="gpt-3.5-turbo",api_key=key)

    # Create a db retriever by loading the ChromaDB
    retriever = Chroma(persist_directory="chroma-laws", embedding_function=OpenAIEmbeddings(api_key=key))
    retriever = retriever.as_retriever()

    # Instructions template
    instruct_system_prompt = (
        "Du bist ein Chat-Assistent für die rechtliche Beratung."
        "Nutze der abgerufene Konetxt um die Frage zu beantworten. Falls der Kontext nicht genügend Information hat, sag das."
        "Falls du nicht verstehst, dann frage was dir nicht klar ist."
        "versuche weinger sätze zu nutzen wenn es möglich ist, aber sei frei die Gesetze vollständig zu zitieren."
        "Fühl dich frei gesprächig und geistreich zu sein. "
        "\n\n"
        "{context}")
    
    instruct_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", instruct_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])


    # Compile the final chain and return it
    qa_chain = create_stuff_documents_chain(llm, instruct_prompt)
    rag_chain = create_retrieval_chain(retriever, qa_chain)
    
    return rag_chain


def user_in(uin, rag_chain, history):
    """ Return the output generated by GPT.

    Args:
        uin (str): user input.
        rag_chain (Runnable Sequence): llm chain.
        history (list): chat memory list.
    Returns:
        str: Genenerated response
    """
    result = rag_chain.invoke({"input": uin, "chat_history" : history})["answer"]
    
    # Update the chat history with new question and response
    history.extend([HumanMessage(content=uin),AIMessage(content=result),])
    return result
    


def main(memory=memory):
    st.title("Les Gutmann")

    # Create and store in streamlit cache
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    if "rag" not in st.session_state:
        st.session_state.rag = init_chain()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # update the chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt:= st.chat_input("Servus!"):
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        st.session_state.messages.append({"role":"user", "content": prompt})
        response = user_in(prompt, st.session_state.rag, st.session_state.chat_history[-memory:]) 
        
        with st.chat_message("assistant"):
            st.markdown(response)
        st.session_state.messages.append({"role" : "assistant", "content": response})


if __name__ == "__main__":   
    main()